from multiprocessing import Queue as MPQueue
from queue import Queue as ThreadQueue
from threading import Thread
from time import localtime, strftime, sleep
from typing import List, Union

from loguru import logger as loguru_logger


class TerminationSignal:
    """ç”¨äºæ ‡è®°ä»»åŠ¡é˜Ÿåˆ—ç»ˆæ­¢çš„å“¨å…µå¯¹è±¡"""

    pass


class TaskError(Exception):
    """ç”¨äºæ ‡è®°ä»»åŠ¡æ‰§è¡Œé”™è¯¯çš„å¼‚å¸¸ç±»"""

    pass


class TaskLogger:
    """
    ç”¨äºè®°å½•ä»»åŠ¡æ‰§è¡Œæ—¥å¿—çš„ç±»
    """

    def __init__(self, level="INFO"):
        self.logger = loguru_logger

        self.logger.remove()  # remove the default handler
        now_time = strftime("%Y-%m-%d", localtime())
        self.logger.add(
            f"logs/task_logger({now_time}).log",
            format="{time:YYYY-MM-DD HH:mm:ss} {level} {message}",
            level=level,
            enqueue=True,
        )

    def start_manager(self, func_name, task_num, execution_mode, worker_limit):
        start_text = f"'{func_name}' start {task_num} tasks by {execution_mode}"
        start_text += (
            f"({worker_limit} workers)." if execution_mode != "serial" else "."
        )
        self.logger.info(start_text)

    def end_manager(
        self,
        func_name,
        execution_mode,
        use_time,
        success_num,
        failed_num,
        duplicated_num,
    ):
        self.logger.info(
            f"'{func_name}' end tasks by {execution_mode}. Use {use_time:.2f} second. {success_num} tasks successed, {failed_num} tasks failed, {duplicated_num} tasks duplicated."
        )

    def start_stage(self, stage_name, func_name, execution_mode, worker_limit):
        start_text = (
            f"The {stage_name} in '{func_name}' start tasks by {execution_mode}"
        )
        start_text += (
            f"({worker_limit} workers)." if execution_mode != "serial" else "."
        )
        self.logger.info(start_text)

    def end_stage(
        self,
        stage_name,
        func_name,
        execution_mode,
        use_time,
        success_num,
        failed_num,
        duplicated_num,
    ):
        self.logger.info(
            f"The {stage_name} in '{func_name}' end tasks by {execution_mode}. Use {use_time:.2f} second. {success_num} tasks successed, {failed_num} tasks failed, {duplicated_num} tasks duplicated."
        )

    def start_tree(self, stage_structure):
        self.logger.info(f"Starting TaskTree stages. Tree structure:")
        for structure in stage_structure:
            self.logger.info(f"{structure}")

    def end_tree(self, use_time):
        self.logger.info(f"TaskTree end. Use {use_time:.2f} second.")

    def task_success(self, func_name, task_info, execution_mode, result_info, use_time):
        self.logger.success(
            f"In '{func_name}', Task {task_info} completed by {execution_mode}. Result is {result_info}. Used {use_time:.2f} seconds."
        )

    def task_retry(self, func_name, task_info, retry_times):
        self.logger.warning(
            f"In '{func_name}', Task {task_info} failed {retry_times} times and will retry."
        )

    def task_error(self, func_name, task_info, exception):
        self.logger.error(
            f"In '{func_name}', Task {task_info} failed and can't retry: ({type(exception).__name__}){exception}."
        )

    def task_duplicate(self, func_name, task_info):
        self.logger.success(
            f"In '{func_name}', Task {task_info} has been duplicated."
        )

    def splitter_success(self, func_name, task_info, split_count, use_time):
        self.logger.success(
            f"In '{func_name}', Task {task_info} has split into {split_count} parts. Used {use_time:.2f} seconds."
        )


class BroadcastQueueManager:
    def __init__(
        self,
        input_queue: Union[MPQueue, ThreadQueue],
        target_queues: List[Union[MPQueue, ThreadQueue]],
        func_name: str,
    ):
        """
        å¹¿æ’­é˜Ÿåˆ—ç®¡ç†å™¨
        :param input_queue: æºè¾“å…¥é˜Ÿåˆ—
        :param target_queues: ç›®æ ‡é˜Ÿåˆ—åˆ—è¡¨
        """
        self.input_queue = input_queue
        self.target_queues = target_queues
        self.func_name = func_name

    def start(self):
        """å¼€å§‹å¹¿æ’­çº¿ç¨‹"""
        self.thread = Thread(target=self.broadcast)
        self.thread.start()

    def stop(self):
        """åœæ­¢å¹¿æ’­çº¿ç¨‹"""
        self.thread.join()

    def broadcast(self):
        """å°†è¾“å…¥é˜Ÿåˆ—çš„æ•°æ®å¹¿æ’­åˆ°ç›®æ ‡é˜Ÿåˆ—"""
        while True:
            try:
                item = self.input_queue.get()  # ä»è¾“å…¥é˜Ÿåˆ—è·å–æ•°æ®
                if isinstance(item, TerminationSignal):  # æ£€æµ‹åˆ°ç»ˆæ­¢ä¿¡å·æ—¶å¹¿æ’­å¹¶é€€å‡º
                    break
                self._broadcast_to_all(item)
            except Exception as e:
                task_logger.logger.error(
                    f"{self.func_name} broadcast thread error: {e}"
                )
        self._broadcast_to_all(TERMINATION_SIGNAL)

    def _broadcast_to_all(self, item):
        """å¹¿æ’­æ•°æ®åˆ°æ‰€æœ‰ç›®æ ‡é˜Ÿåˆ—"""
        task_logger.logger.trace(
            f"{self.func_name} broadcasting {item} to all target queues."
        )
        for queue in self.target_queues:
            queue.put(item)


def monitor_processes(processes, poll_interval=3):
    while True:
        all_done = True
        for p in processes:
            p.join(timeout=0.1)  # ğŸ‘ˆ å¼ºåˆ¶åŒæ­¥çŠ¶æ€
            if p.exitcode is None:
                task_logger.logger.debug(f"[MONITOR] {p.name} still running")
                all_done = False
            elif p.exitcode == 0:
                task_logger.logger.debug(f"[MONITOR] {p.name} completed successfully")
            else:
                task_logger.logger.debug(f"[MONITOR] âŒ {p.name} crashed with exitcode {p.exitcode}")
        if all_done:
            task_logger.logger.debug("[MONITOR] âœ… All processes done.")
            break
        sleep(poll_interval)


TERMINATION_SIGNAL = TerminationSignal()
task_logger = TaskLogger("DEBUG")
